{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 3: Language Modeling\n",
    "=============\n",
    "In this problem set, your objective is to train a language model, evaluate it and explore how it can be used for language generation. Towards that end you will:\n",
    "\n",
    "- Train an n-gram language model.\n",
    "- Use that language model to generate representative sentences.\n",
    "- Study the effect of training data size, and language model complexity (n-gram size), on the modeling capacity of a language model.\n",
    "\n",
    "- **For this assignment, submit ```lab3.py``` on Gradescope.**\n",
    "- In order to test the lab you can run ```python run_tests.py``` or ```python run_tests.py -j``` (more detailed information)\n",
    "- In order to install the correct dependencies you can run ```pip install -r requirements.txt```\n",
    "\n",
    "Total points: 90 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "In order to develop this assignment, you will need [python 3.6](https://www.python.org/downloads/) and the following libraries. Most if not all of these are part of [anaconda](https://www.continuum.io/downloads), so a good starting point would be to install that.\n",
    "\n",
    "- [jupyter](http://jupyter.readthedocs.org/en/latest/install.html)\n",
    "- [nosetests](https://nose.readthedocs.org/en/latest/)\n",
    "- [nltk](https://www.nltk.org)\n",
    "\n",
    "Here is some help on installing packages in python: https://packaging.python.org/installing/. You can use ```pip --user``` to install locally without sudo. We have also provided a requirements.txt file with the correct packages and their respective versions, so you can also run ```pip install -r requirements.txt``` to install the correct dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload\n",
    "from collections import defaultdict\n",
    "import lab3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "My Python version\npython: 3.9.1 (default, Dec 28 2020, 11:25:16) \n[Clang 12.0.0 (clang-1200.0.32.28)]\n"
     ]
    }
   ],
   "source": [
    "print('My Python version')\n",
    "\n",
    "print('python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "My library versions\nnose: 1.3.7\nnltk: 3.6.1\n"
     ]
    }
   ],
   "source": [
    "print('My library versions')\n",
    "\n",
    "print('nose: {}'.format(nose.__version__))\n",
    "print('nltk: {}'.format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your libraries are the right version, run:\n",
    "\n",
    "`nosetests tests/test_environment.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training a language model\n",
    "\n",
    "Let us first train a 3-gram language model. We need a monolingual corpus, which we will get using nltk.\n",
    "\n",
    "Total: 40 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first extract from nltk's reuters corpus, 2 corpora in 2 different domains (here, subject areas), the food industry and the natural resources industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "food = ['barley', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copra-cake', 'grain', 'groundnut', 'groundnut-oil', 'potato', 'soy-meal', 'soy-oil', 'soybean', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'veg-oil', 'wheat']\n",
    "natural_resources = ['alum', 'fuel', 'gas', 'gold', 'iron-steel', 'lead', 'nat-gas', 'palladium', 'propane', 'tin', 'zinc']\n",
    "corpus = nltk.corpus.reuters\n",
    "food_corpus = corpus.raw(categories=food)\n",
    "natr_corpus = corpus.raw(categories=natural_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " --    2.54\n   Evnsvlle    1.54  2.04  1.50  2.17  4.90  2.61\n   Cinci       1.52  2.04  1.50  2.17  4.85  2.58\n   Mpls        1.34  1.75  1.50  1.85  4.68  --\n   Balt/Nor/\n   Phil        1.70  1.80  --    --    4.98  3.12\n   KC          1.49  1.56  1.64  --    4.76  2.58\n   St Lo       1.54  --    1.66  --    4.90  2.91\n   Amarlo/\n   Lubbck      1.84  1.40  --    --    4.75  2.92\n   Lou Gulf    1.73  --    --    --    5.05  3.12\n   Port/\n   Seattle     1.87  2.10  1.68  --    --    --\n   Stockton    2.18  2.23  2.10  --    --    4.00\n   LA          2.54  2.50  --    --    --    4.38\n   Duluth      1.34  1.75  1.50  1.85  4.68  --\n   Tex Gulf    1.73  1.48  1.73  --    5.05  3.12\n  \n\nVIETNAM'S ARMY ORDERED TO GROW MORE FOOD\n  Vietnam has ordered its army to grow\n  more food to ease shortages and meet economic recovery goals\n  set for 1990.\n      The army newspaper Quan Doi Nhan Dan, monitored here, said\n  soldiers must work harder to care for rice, vegetables and\n  other crops endangered by the present unusually hot weather.\n      The paper said the 1.6-mln strong regular army contributed\n  less than one pct to the nation's 18.2 mln tonne food output.\n      North Vietnam has set a 1990 food target of 23 to 24 mln\n  tonnes.\n  \n\nEC SUGAR TENDER SEEN AS FURTHER CONCESSION\n  The rebates granted at yesterday's EC\n  sugar tender represent a further concession to producers'\n  complaints that they are losing money on exports outside the\n  bloc, trade sources said.\n      They said the maximum rebate of 45.678 European currency\n  Units (Ecus) per 100 kilos was 0.87 Ecus below what producers\n  claim is needed to obtain the equivalent price to that offered\n  for sales into intervention.\n      The rebate at last week's tender was 1.3 Ecus short of the\n  level producers thought necessary and that of the previous week\n  was 2.5 Ecus below this level.\n      But the sources said producers who have offered a total of\n  854,000 tonnes of sugar into intervention in an apparent\n  attempt to persuade the Commission to set higher maximum\n  rebates have given no formal indication to the Commission that\n  they intend to withdraw these offers.\n      The French and German operators involved would be able to\n  withdraw the offers up to five weeks after April 1 when the\n  sugar will officially enter intervention stores.\n      The five-week period is the normal delay between sugar\n  going into intervention and payment being made for it.\n      EC officials have said that if the Commission has to buy\n  the sugar, it is determined immediately to resell it, a move\n  which could drive down market prices further.\n  \n\nU.K. OILMEAL/VEG OIL PRODUCTION ROSE IN 1986\n  The U.K. Produced 820,400 tonnes of\n  oilcake and meal and 431,000 tonnes of crude vegetable oil in\n  calendar 1986, Ministry of Agriculture figures show.\n      They compare with 785,800 tonnes of oilcake and meal and\n  407,400 tonnes of crude vegetable oil produced in 1985.\n      Total oilseeds crushed rose to 1.27 mln tonnes from 1.21\n  mln in 1985.\n  \n\nCOFFEE PRICE FALL SHORT TERM - DUTCH ROASTERS\n  This morning's sharp decline in coffee\n  prices, following the breakdown late last night of negotiations\n  in London to reintroduce International Coffee Organization,\n  ICO, quotas, will be short-lived, Dutch roasters said.\n      \"The fall is a technical and emotional reaction to the\n  failure to agree on reintroduction of ICO export quotas, but it\n  will not be long before reality reasserts itself and prices\n  rise again,\" a spokesman for one of the major Dutch roasters\n  said.\n      \"The fact is that while there are ample supplies of coffee\n  available at present, there is a shortage of quality,\" he said.\n      \"Average prices fell to around 110 cents a lb following the\n  news of the breakdown but we expect them to move back again to\n  around 120 cents within a few weeks,\" the roaster added.\n      Dutch Coffee Roasters' Association secretary Jan de Vries\n  said although the roasters were disappointed at the failure of\n  consumer and producer ICO representatives to agree on quota\n  reintroduction, it was equally important that quotas be\n  reallocated on a more equitable basis.\n      \"There is no absolute need for quotas at this moment because\n  the market is well balanced and we must not lose this\n  opportunity to renegotiate the coffee agreement,\" he said.\n      \"There is still a lot of work to be done on a number of\n  clauses of the International Coffee Agreement and we would not\n  welcome quota reintroduction until we have a complete\n  renegotiation,\" de Vries added.\n      With this in mind, and with Dutch roasters claiming to have\n  fairly good forward cover, the buying strategy for the\n  foreseeable future would probably be to buy coffee on a\n  hand-to-mouth basis and on a sliding scale when market prices\n  were below 120 cents a lb, roasters said.\n  \n\nIWC ups Soviet grain 1986/87 import estimate three mln tonnes to 29 mln - official\n\n  IWC ups Soviet grain 1986/87 import estimate three mln tonnes to 29 mln - official\n  \n\nIWC lifts 1986/87 world wheat, coarse grain estimate one mln tonnes to record 1,377 mln\n\n  IWC lifts 1986/87 world wheat, coarse grain estimate one mln tonnes to record 1,377 mln\n  \n\nIWC LIFTS WORLD GRAIN OUTPUT ESTIMATE TO RECORD\n  The International Wheat Council (IWC)\n  lifted its estimate for 1986/87 world wheat and coarse grain\n  production by one mln tonnes to a record 1,377 mln, compared\n  with 1,351 mln tonnes the previous season.\n      In its monthly market report, the IWC said it is leaving\n  unchanged its forecast of world wheat production for the coming\n  1987/88 season at between 520 and 530 mln tonnes against a\n  record 534 mln in 1986/87. The one mln tonne upward revision in\n  1986/87 wheat production reflects several minor adjustments.\n  The IWC raised the 1986/87 coarse grain trade figure two mln to\n  87 mln tonnes. It left wheat trade unchanged at 86 mln.\n      The IWC 1986/87 estimate for world trade in wheat and\n  coarse grain is thus estimated two mln tonnes higher at 173 mln\n  against 169 mln the previous season with the forecast three mln\n  rise in Soviet imports offset by small reductions elsewhere.\n      The IWC said the area harvested for wheat in 1987/88 is\n  likely to be down from last year as low world prices and\n  restrictive national policies measures begin to take effect.\n      At least four of the five major exporters expect to see a\n  drop in wheat sowings without offset in other countries. There\n  is still potential for even higher average wheat yields but the\n  IWC said there are increasing signs world output may level off.\n      Although it is still early to assess the coarse grain\n  outlook, the IWC said barley acreage is likely to fall in the\n  European Community but increase in Canada. U.S. Maize area is\n  expected lower but oat sowings could rise.\n      After damage to its maize crop last year, the Soviet Union\n  plans to expand this area by as much as 50 pct to over six mln\n  hectares in a year when many frost damaged wheat fields are\n  likely to be resown to this and other spring crops. Improved\n  weather and a further increase in the use of intensive\n  cultivation methods could therefore see a marked rise in Soviet\n  maize output in 1987, the IWC said.\n      Any reduction in world coarse grain output would be\n  bolstered by the large carryover stocks from 1986/87, the IWC\n  said.\n      It left its estimates of wheat and coarse grain stocks at\n  endof different marketing years unchanged at 178 and 210 mln\n  tonnes, respectively, against 160 and 167 mln a year earlier.\n      After record world durum wheat production of 218.8 mln\n  tonnes last season, the IWC said there are already signs of\n  another large crop this coming season with higher output\n  expected in the EC, Canada, the U.S. And North Africa.\n  \n\nIWC SAYS EFFECT OF LOWER SUPPORT PRICES LIMITED\n  Efforts by governments to control wheat\n  surpluses by cutting support prices have met with only partial\n  success, the International Wheat Council (IWC) says in its\n  latest monthly report.\n      Faster results could be achieved by a policy of reducing\n  both price and areas, as employed in the United States, the IWC\n  says in a survey of support prices in the five main wheat\n  exporters - Argentina, Australia, Canada, the EC and the U.S.\n      In some countries, for example Australia and Argentina,\n  which are highly dependent on wheat shipments for export\n  income, there may be problems in reducing production.\n      A policy of cutting wheat production could lead to\n  unemployment, with job prospects outside agriculture limited.\n  Alternative crops may offer inferior returns which could then\n  lead to lost export revenue and balance of payments problems.\n      The IWC outlines three courses of action open to\n  governments in wheat exporting countries.\n      They could continue to support prices in the hope that when\n  the world economy improves demand for wheat will rise and\n  surpluses wil be reduced or eliminated.\n      Alternatively, support could be limited to wheat which\n  could be easily sold, without needing to be stored for a long\n  period.\n      This option may prove to be the most politically\n  unattractive and would result in many producers abandoning\n  wheat production, the report said.\n      The third option would be for governments to distinguish\n  between the commercial and social aspects of agriculture,\n  possibly varying support prices according to farm size or\n  overall production.\n      The IWC review covers support prices in the major exporting\n  countries since 1982. At some time during that period all the\n  producers cut support prices in response to growing surpluses.\n      These changes did not always result in lower export\n  subsidies as on several occasions currency fluctuations more\n  than offset lower prices in the domestic currency.\n      For example between 1985/86 and 1986/87 the EC intervention\n  price for bread wheat fell from 209.30 to 179.44 European\n  currency units (Ecus). It dollar terms, the currency in which\n  most export transactions are denominated, the intervention\n  price however rose to 193 dlrs from 168. The high cost of\n  supporting farm prices has put a strain on national exchequers\n  and some governments are now searching for ways to cut\n  expenditure, the report says.\n      The proportion of world wheat output produced by the five\n  major exporters declined in the period covered by the survey\n  from 40 pct in 1982 to 35 pct in 1987. This was partly due to\n  increased production in China and India.\n      The period saw an upward trend in yields, although this was\n  countered in the Argentina, the U.S. And Australia by lower\n  acreages.\n      In Argentina a reduction in the sown area of about 20 per\n  cent was put down to low prices causing producers to switch to\n  other enterprises, particularly livestock while lower U.S.\n  Acreages are attributed to official incentives.\n  \n\nFRENCH FREE MARKET CEREAL EXPORT BIDS DETAILED\n  French operators have requested licences\n  to export 675,500 tonnes of maize, 245,000 tonnes of barley,\n  22,000 tonnes of soft bread wheat and 20,000 tonnes of feed\n  wheat at today's European Community tender, traders said.\n      Rebates requested ranged from 127.75 to 132.50 European\n  Currency Units a tonne for maize, 136.00 to 141.00 Ecus a tonne\n  for barley and 134.25 to 141.81 Ecus for bread wheat, while\n  rebates requested for feed wheat were 137.65 Ecus, they said.\n  \n\nMADAGASCAR COCOA PRODUCTION ESTIMATED HIGHER\n  Madagascar's cocoa production is\n  estimated 13 pct higher this year at 2,720 tonnes, up from\n  2,400 in 1986, Agriculture Ministry officials said.\n      This improvement reflects the government's efforts over the\n  last seven years to extend existing cocoa plantations and plant\n  new higher yielding varieties, particularly at the northern tip\n  of the island, they said.\n      Last year, Madagascar exported 2,189 tonnes of high quality\n  cocoa, up from 1,624 in 1985, the Trade Ministry said.\n      This year's exports are estimated at 2,400 tonnes.\n  \n\nDUTCH GRAIN LEVY TEST CASE TO START IN APRIL\n  A large Dutch animal feed compounder\n  will begin formal legal proceedings early next month as a test\n  case on the way the EC grain co-responsibility levy is applied,\n  a spokesman for Dutch grain and feed trade association, Het\n  Comite, told Reuters.\n      Het Comite has been co-ordinating national actions against\n  alleged distortions caused by currency factors in the levy and,\n  since December, has lodged more than 80 individual cases with\n  the Business Appeal Court in The Hague.\n      The basic complaint is that the levy does not take account\n  of currency cross-rates of exchange and therefore compounders\n  in countries with strong currencies may have to pay more in\n  their own currency than is paid to them by producers in another\n  country.\n      Het Comite has obtained a temporary agreement that\n  companies can pay the amount they receive toward the levy\n  rather than paying a full guilder amount to the Dutch grain\n  commodity board.\n      The spokesman said Het Comite will provide financial and\n  legal backing to the test case in the Business Administration\n  Court in the Hague. Oral proceedings are to begin on April 10.\n      The spokesman said Het Comite finally selected the company\n  for the test case from among the 80 lodged \"because the bill\n  (the firm) received from the commodity board for payment of the\n  levy contained significant currency distortions and involved\n  grain from a wide variety of origins.\" The name of the company\n  is not being made public.\n      The Administration Court is not expected to make a final\n  ruling on the case in the near future. The Het Comite spokesman\n  said it was very likely it would refer questions to the Appeal\n  Court in Luxembourg, and \"as a result it could easily be another\n  nine to 12 months before the matter is finally resolved.\"\n      Meanwhile, the actions by Dutch animal feed compounders are\n  putting pressure on the commodity board to urge the Dutch\n  government to follow through on earlier statements and seek a\n  complete review in Brussels of the way in which the levy is\n  collected, the spokesman said.\n      Het Comite, as a member of FEFAC, the association of\n  European animal feed manufacturers, is also a party to actions\n  protesting the whole levy in the Luxembourg appeal court.\n  \n\nBRAZIL SOY HARVEST 13 PCT COMPLETE - NEWSLETTER\n  Brazil's soybean harvest was 13\n  pct complete by March 20, the Safras e Mercado newsletter said.\n      This compares with an historic average for this time of\n  year of 20 pct.\n      The newsletter gave the following figures for the progress\n  of the harvest in the main producer states:\n      Parana: 40 pct\n      Mato Grosso do Sul: 15 pct\n      Mato Grosso: five pct\n      Rio Grande do Sul: two pct\n  \n\nU.S. SENATE HITS EC OILS TAX, VOWS RETALIATION\n  The Senate voted to condemn the\n  proposed European common market tax on vegetable and marine\n  fats and oils and said it would result in retaliation.\n      The non-binding Senate resolution, a sense of Senate\n  sentiment, was approved on a 99 to 0 vote.\n      \"The administration should communicate to the European\n  Community the message that the United States will view the\n  establishment of such a tax as inconsistent with the European\n  Community's obligations under the General Agreement on Tariffs\n  and Trade that will result in the adoption of strong and\n  immediate countermeasures,\" the resolution stated.\n      The resolution said the European Community Commission has\n  proposed establishing a consumption tax on vegetable and fish\n  oils and fats in conjunction with the setting of farm prices\n  for the 1987/1988 EC marketing year.\n      The Senate said the tax would amount to almost 90 pct of\n  the current price of soyoil and \"have a restrictive effect\" on\n  U.S. exports of soybeans and vegetable oils to the EC.\n      It would be \"blatantly inconsistent\" with obligations of the\n  EC under the General Agreement on Tariffs and Trade, GATT, the\n  resolution said, and \"constitute another egregious attempt\" to\n  impose EC agricultural costs on trading partners.\n  \n\nLIBYA REPORTEDLY BOUGHT WHITE SUGAR\n  Libya is reported to have recently\n  bought two cargoes of white sugar from operators at around\n  229/230 dlrs a tonne cost and freight, traders said.\n      The shipment period required was not specified.\n  \n\nCOCOA COUNCIL HEAD TO PRESENT BUFFER COMPROMISE\n  International Cocoa Organization, ICCO,\n  council chairman Denis Bra Kanon will present a compromise\n  proposal on buffer stock rules to producer and consumer\n  delegates either later today or tomorrow morning, delegates\n  said.\n      Bra Kanon held private bilateral consultations with major\n  producers and consumers this morning to resolve outstanding\n  differences, mostly on the issues of how much non-member cocoa\n  the buffer stock can purchase and price differentials for\n  different varieties.\n      Delegates were fairly confident the differences could be\n  worked out in time to reach agreement tomorrow.\n      Some consuming member nations, including Britain and\n  Belgium, favour the buffer stock buying more than 10 pct\n  non-member cocoa, delegates have said.\n      The consumers argue that buying cheaper, lower quality\n  non-member cocoas, particularly Malaysian, will most\n  effectively support prices because that low quality cocoa is\n  currently pressuring the market.\n      Producers, meanwhile, say non-member cocoa should make up\n  at most a very small percentage of the buffer. They say\n  Malaysia should not be able to benefit from the ICCO unless it\n  is a member, and if the buffer stock bought Malaysian cocoa\n  Malaysia would have no incentive to join, delegates said.\n      As to differentials, Ghana apparently wanted a higher\n  differential for its cocoa than is outlined in the most recent\n  proposal, so it would have a better chance of having its cocoa\n  bought for the buffer stock, producer delegates said.\n      Some consumers wanted differentials to be adjusted in a way\n  that would not promote buffer stock purchases of the more\n  expensive cocoas, such as Ghanaian and Brazilian, they said.\n      Other technical points need to be sorted out, including\n  limits on how much cocoa the buffer stock manager can buy in\n  nearby, intermediate and forward positions and the consequent\n  effect on prices in the various deliveries, delegates said.\n  \n\nEC GRANTS EXPORT LICENCES 197,000 TONNES  FREE MARKET MAIZE, ZERO BARLEY - PARIS TRADERS\n\n  EC GRANTS EXPORT LICENCES 197,000 TONNES  FREE MARKET MAIZE, ZERO BARLEY - PARIS TRADERS\n  \n\nU.S. SENATE HITS EC OILS TAX, VOWS RETALIATION\n  The Senate voted to condemn the\n  proposed European common market tax on vegetable and fish fats\n  and oils and said it would result in retaliation.\n      The non binding Senate resolution, a sense of Senate\n  sentiment, was approved on a 99 to 0 vote.\n      \"The administration should communciate to the European\n  Community the message that the United States will view the\n  establishment of such a tax as inconsistent with the European\n  Community's obligations under the General Agreement on Tariffs\n  and Trade that will result in the adoption of strong and\n  immediate countermeasures,\" the resolution stated.\n  \n\nSENATE SEEKS U.S. PROBE OF CANADIAN CORN LEVY\n  The Senate voted unanimously to seek\n  an expedited U.S. probe of Canadian tariffs on corn imports to\n  determine if the United States should retaliate.\n      By 99 to 0, the Senate went on record against the 84.9\n  cents per bushel tariff approved by the Canadian Import\n  Tribunal.\n      The non binding measure asked for a probe by the U.S. Trade\n  Representative to determine within 30 days whether the tariff\n  violates the General Agreement on Tariffs and Trade, and if so\n  recommend within 60 days to President Reagan retaliatory action\n  against Canada.\n  \n\n\n"
     ]
    }
   ],
   "source": [
    "print(food_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Your first task is to tokenize the raw text into a list of sentences, which are in turn a list of words. No need for any other kind of preprocessing such as lowercasing.\n",
    "\n",
    "- **Deliverable 1.1**: Complete the function `lab3.tokenize`. (5 points)\n",
    "- **Test**: `tests/test_visible.py:test_d1_1_tk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xietian/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_corpus_tk = lab3.tokenize_corpus(food_corpus)\n",
    "natr_corpus_tk = lab3.tokenize_corpus(natr_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "Your second task is to pad your sentences with the start-of-sentence symbol `'<s>'` and end-of-sentence symbol `'</s>'`. These symbols are necessary to model the probability of words that usually start a sentence and those that usually end a sentence.\n",
    "\n",
    "- **Deliverable 1.2**: Complete the function `lab3.pad`. (5 points)\n",
    "- **Test**: `tests/test_visible.py:test_d1_2_pad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_corpus_tk_pd = lab3.pad_corpus(food_corpus_tk)\n",
    "natr_corpus_tk_pd = lab3.pad_corpus(natr_corpus_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Your third task is to split the corpora into train, for training the language model, and test, for testing the language model. We will go with the traditional 80% (train), 20% (test) split. The first `floor(0.8*num_of_tokens)` should constitute the training corpus, and the rest should constitute the test corpus.\n",
    "\n",
    "- **Deliverable 1.3**: Complete the function `lab3.split_corpus`. (5 points)\n",
    "- **Test**: `tests/test_visible.py:test_d1_3_spc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_corpus_tr, food_corpus_te = lab3.split_corpus(food_corpus_tk_pd)\n",
    "natr_corpus_tr, natr_corpus_te = lab3.split_corpus(natr_corpus_tk_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['<s>', 'CHINA', 'DAILY', 'SAYS', 'VERMIN', 'EAT', '7-12', 'PCT', 'GRAIN', 'STOCKS', 'A', 'survey', 'of', '19', 'provinces', 'and', 'seven', 'cities', 'showed', 'vermin', 'consume', 'between', 'seven', 'and', '12', 'pct', 'of', 'China', \"'s\", 'grain', 'stocks', ',', 'the', 'China', 'Daily', 'said', '.', '</s>'], ['<s>', 'It', 'also', 'said', 'that', 'each', 'year', '1.575', 'mln', 'tonnes', ',', 'or', '25', 'pct', ',', 'of', 'China', \"'s\", 'fruit', 'output', 'are', 'left', 'to', 'rot', ',', 'and', '2.1', 'mln', 'tonnes', ',', 'or', 'up', 'to', '30', 'pct', ',', 'of', 'its', 'vegetables', '.', '</s>']]\n\nass\n\n['<s>', 'CHINA', 'DAILY', 'SAYS', 'VERMIN', 'EAT', '7-12', 'PCT', 'GRAIN', 'STOCKS', 'A', 'survey', 'of', '19', 'provinces', 'and', 'seven', 'cities', 'showed', 'vermin', 'consume', 'between', 'seven', 'and', '12', 'pct', 'of', 'China', \"'s\", 'grain', 'stocks', ',', 'the', 'China', 'Daily', 'said', '.', '</s>', '<s>', 'It', 'also', 'said', 'that', 'each', 'year', '1.575', 'mln', 'tonnes', ',', 'or', '25', 'pct', ',', 'of', 'China', \"'s\", 'fruit', 'output', 'are', 'left', 'to', 'rot', ',', 'and', '2.1', 'mln', 'tonnes', ',', 'or', 'up', 'to', '30', 'pct', ',', 'of', 'its', 'vegetables', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "flat_list = [item for sublist in food_corpus_tr[0:2] for item in sublist]\n",
    "# for i in range(len(flat_list)):\n",
    "#     print(food_corpus_tr[i])\n",
    "#     print(flat_list[i], end=\" \")\n",
    "#     print(\"\\nass\\n\")\n",
    "print(food_corpus_tr[0:2])\n",
    "print(\"\\nass\\n\")\n",
    "print(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['<s>', 'CHINA', 'DAILY', 'SAYS', 'VERMIN', 'EAT', '7-12', 'PCT', 'GRAIN', 'STOCKS', 'A', 'survey', 'of', '19', 'provinces', 'and', 'seven', 'cities', 'showed', 'vermin', 'consume', 'between', 'seven', 'and', '12', 'pct', 'of', 'China', \"'s\", 'grain', 'stocks', ',', 'the', 'China', 'Daily', 'said', '.', '</s>', '<s>', 'It', 'also', 'said', 'that', 'each', 'year', '1.575', 'mln', 'tonnes', ',', 'or', '25', 'pct', ',', 'of', 'China', \"'s\", 'fruit', 'output', 'are', 'left', 'to', 'rot', ',', 'and', '2.1', 'mln', 'tonnes', ',', 'or', 'up', 'to', '30', 'pct', ',', 'of', 'its', 'vegetables', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "for i in food_corpus_tr[0:2]:\n",
    "    word_list.extend(i)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into n-grams\n",
    "\n",
    "Your fourth task is to count n-grams in the text up to a specific order.\n",
    "\n",
    "- **Deliverable 1.4**: Complete the function `lab3.count_ngrams`. (20 points)\n",
    "- **Test**: `tests/test_visible.py:test_d1_4_cn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_ngrams, food_vocab = lab3.count_ngrams(food_corpus_tr, 3)\n",
    "natr_ngrams, natr_vocab = lab3.count_ngrams(natr_corpus_tr, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating n-gram probability\n",
    "\n",
    "Your last task in this part of the problem set is to estimate the n-gram probabilities p(w_i|w_{i-n+1}, w_{i-n+2}, .., w_{i-1}), with no smoothing. For the purposes of this exercise we will use the maximum likelihood estimate and perform no smoothing. \n",
    "\n",
    "- **Deliverable 1.5**: Complete the function `lab3.estimate`. (5 points)\n",
    "- **Test**: `tests/test_visible.py:test_d1_5_es`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.25\n0.5\n"
     ]
    }
   ],
   "source": [
    "print(lab3.estimate(food_ngrams, ['palm'], ['producer', 'of']))\n",
    "print(lab3.estimate(natr_ngrams, ['basis'], ['tested', 'the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application: the speech recognition task takes human voice as its input and outputs text. If the pronunciation of two words are similar, Language Model can help decide which word to choose! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11\n0\n"
     ]
    }
   ],
   "source": [
    "print(food_ngrams[('there', 'is', 'no')])\n",
    "print(food_ngrams[('their', 'is', 'no')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the count of 'there is no' and 'their is no', which word ('there' or 'their') is more likely to be taken as the output? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Model is not only helpful in speech recogition, but text generation (*e.g.*, machine translation, summarization, image captioning), spelling correction and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a language model\n",
    "\n",
    "Now we will combine everything together and train our language model! One way to see what the language model has learned is to see the sentences it can generate.\n",
    "\n",
    "For the sake of simplicity, and for the purposes of later parts in this problem set, we use nltk's lm module to train a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "size_ngram = 3\n",
    "\n",
    "food_train, food_vocab = padded_everygram_pipeline(size_ngram, food_corpus_tk[:int(0.8*len(food_corpus_tk))])\n",
    "natr_train, natr_vocab = padded_everygram_pipeline(size_ngram, natr_corpus_tk[:int(0.8*len(natr_corpus_tk))])\n",
    "\n",
    "food_test = sum([['<s>'] + x + ['</s>'] for x in food_corpus_tk[int(0.8*len(food_corpus_tk)):]],[])\n",
    "natr_test = sum([['<s>'] + x + ['</s>'] for x in natr_corpus_tk[int(0.8*len(natr_corpus_tk)):]],[])\n",
    "\n",
    "food_lm = Laplace(size_ngram)\n",
    "natr_lm = Laplace(size_ngram)\n",
    "\n",
    "food_lm.fit(food_train, food_vocab)\n",
    "natr_lm.fit(natr_train, natr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask our language model to generate a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['<s>', '<s>', '<s>', 'GHANA', 'COCOA', 'PURCHASES', '1,323', 'TONNES', 'IN', '1987/88']\n['<s>', '<s>', '<s>', 'HAITI', 'ANNOUNCES', 'FIND', 'OF', 'ORE-RICH', 'GOLD', 'FIELD']\n"
     ]
    }
   ],
   "source": [
    "# This might take some time\n",
    "n_words = 10\n",
    "print(food_lm.generate(n_words, random_seed=3))  # random_seed makes the random sampling part of generation reproducible. \n",
    "print(natr_lm.generate(n_words, random_seed=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluating a language model\n",
    "\n",
    "Next, we evaluate our language models using the perplexity measure, and draw conclusions on how a change of domains (here, subject areas) can affect the performance of a language model. Perplexity measures the language model capacity at predicting sentences in a test corpus.\n",
    "\n",
    "Total: 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 2.1**: Complete the function `lab3.get_perplexity`. (10 points)\n",
    "- **Test**: `tests/test_visible.py:test_d2_1_gp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8682.570675094219\n8858.706404544759\n5614.398479984016\n5663.540343702553\n"
     ]
    }
   ],
   "source": [
    "# This might take some time\n",
    "print(lab3.get_perplexity(food_lm, food_test[:5000]))\n",
    "print(lab3.get_perplexity(food_lm, natr_test[:5000]))\n",
    "print(lab3.get_perplexity(natr_lm, natr_test[:5000]))\n",
    "print(lab3.get_perplexity(natr_lm, food_test[:5000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What observations can you make on the results? Is the domain shift affecting the performance of the language model? What are possible explanations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-75755ce98e18d29b",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your Observation**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data size and model complexity\n",
    "\n",
    "Let us now see how the size of the training data and the complexity of the model we choose affects the quality of our language model.\n",
    "\n",
    "Total: 40 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part we'd like to see the difference between a 2-gram model and a 3-gram model. Typically, with a larger n, the n-gram model gives us more information about the word sequence and has lower perplexity. \n",
    "\n",
    "For testing, we'll only be considering 5% instead of 20% of the test data for running time purposes. \n",
    "\n",
    "- **Deliverable 3.1**: Complete the function `lab3.vary_ngram`. (40 points)\n",
    "- **Test**: `tests/test_visible.py:test_d3_1_vary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'list'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-61cef77dfec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatr_corpus_tk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnatr_corpus_tk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnatr_corpus_tk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<s>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'</s>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n#####\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "test_corpus = natr_corpus_tk[int(0.8*len(natr_corpus_tk)): int(0.85*len(natr_corpus_tk))][:2]\n",
    "new_test = sum([['<s>'] + x + ['</s>'] for x in test_corpus])\n",
    "print(test_corpus)\n",
    "print(\"\\n#####\\n\")\n",
    "print(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "defaultdict(None, {2: 5425.7474270051935, 3: 5455.361088899949})\n"
     ]
    }
   ],
   "source": [
    "n_gram_orders = [2, 3]\n",
    "\n",
    "train_corpus = natr_corpus_tk[:int(0.8*len(natr_corpus_tk))]\n",
    "test_corpus = natr_corpus_tk[int(0.8*len(natr_corpus_tk)): int(0.85*len(natr_corpus_tk))]\n",
    "\n",
    "results = lab3.vary_ngram(train_corpus, test_corpus, n_gram_orders)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we notice that the 3-gram language model actually performs worse than the 2-gram language model. This is due to the small size of the training corpus. A 3-gram language model is actually too complex of a model for a small training size. If our training data was larger, we would be seeing the opposite. If we trained 1-gram, 2-gram, and 3-gram models on 38 million words from the Wall Street Journal, we will get perplexity of 962, 170, 109 respectively on a test set of 1.5 million words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultdict(None, {2: 5596.7318534048245, 3: 5625.390747181811})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see a few examples of top frequent n-gram examples. Let's start with unigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(',',), ('the',), ('<s>',), ('</s>',), ('.',), ('of',), ('to',), ('and',), ('said',), ('in',), ('a',), ('for',), ('The',), ('from',), ('pct',), ('mln',), ('at',), ('on',), (\"'s\",), ('is',)]\n"
     ]
    }
   ],
   "source": [
    "natr_ngrams, natr_vocab = lab3.count_ngrams(natr_corpus_tr, 3)\n",
    "\n",
    "top_ngram = []\n",
    "count = 0\n",
    "for i in sorted(natr_ngrams.items(), key=lambda x: x[1], reverse=True):\n",
    "    if len(i[0]) == 1:\n",
    "        top_ngram.append(i[0])\n",
    "        count += 1\n",
    "    if count >=20:\n",
    "        break\n",
    "print(top_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think unigram captures any grammatical information? How well do you think unigram captures the language information? \n",
    "\n",
    "Now let's see bigram and trigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('.', '</s>'), ('said', '.'), ('<s>', 'The'), ('in', 'the'), ('of', 'the'), ('&', 'lt'), ('lt', ';'), (',', 'the'), ('said', 'it'), ('said', 'the'), ('<s>', '``'), (',', \"''\"), (',', 'which'), ('to', 'the'), ('for', 'the'), (',', 'a'), ('on', 'the'), (',', 'and'), ('mln', 'dlrs'), ('<s>', 'It')]\n[('said', '.', '</s>'), ('&', 'lt', ';'), ('.', \"''\", '</s>'), ('<s>', 'The', 'company'), ('<s>', 'It', 'said'), ('he', 'said', '.'), ('ounces', 'of', 'gold'), ('year', '.', '</s>'), ('The', 'company', 'said'), ('added', '.', '</s>'), ('oil', 'and', 'gas'), (',', 'it', 'said'), ('pct', '.', '</s>'), (',', \"''\", 'he'), (',', 'he', 'said'), ('it', 'said', '.'), ('sources', 'said', '.'), ('is', 'expected', 'to'), ('<s>', 'He', 'said'), ('the', 'company', 'said')]\n"
     ]
    }
   ],
   "source": [
    "top_ngram = []\n",
    "count = 0\n",
    "for i in sorted(natr_ngrams.items(), key=lambda x: x[1], reverse=True):\n",
    "    if len(i[0]) == 2:\n",
    "        top_ngram.append(i[0])\n",
    "        count += 1\n",
    "    if count >=20:\n",
    "        break\n",
    "print(top_ngram)\n",
    "\n",
    "top_ngram = []\n",
    "count = 0\n",
    "for i in sorted(natr_ngrams.items(), key=lambda x: x[1], reverse=True):\n",
    "    if len(i[0]) == 3:\n",
    "        top_ngram.append(i[0])\n",
    "        count += 1\n",
    "    if count >=20:\n",
    "        break\n",
    "print(top_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with unigram, bigram and trigram can capture more information. \n",
    "Bigram language model can already capture some of the grammatical information, such as 'in the', 'of the'. However, the power of bigram is still limited. \n",
    "The trigram can output more adequate short phrases such as 'ounces of gold', 'The company said', 'oil and gas'. \n",
    "\n",
    "Therefore, typically the n-gram model with a larger n contains more information about the word sequence and thus, has lower perplexity. However, the tradeoff is the computational efficiency and memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}