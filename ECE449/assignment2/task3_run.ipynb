{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "# ECE449 Machine Learning - Assignment 2\n",
    "\n",
    "## Task 3: LeNet with PyTorch\n",
    "\n",
    "\n",
    "[//]: # \"Notebook Created by Jinghua Wang (jinghuawang@intl.zju.edu.cn), last modified on 2020-11-04\"\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "*LeNet* is one of the first published CNNs to capture wide attention for its performance on computer vision image classification tasks.\n",
    "The model was introduced by (and named for) Yann LeCun, then a researcher at AT&T Bell Labs, for the purpose of recognizing handwritten digits in images, in the paper \"Gradient-Based Learning Applied to Document Recognition\" by Yann LeCun, Leon Boottou, Yoshua Bengio, and Patrick Haffner.\n",
    "The paper is available online at: http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf.\n",
    "\n",
    "\n",
    "## LeNet\n",
    "\n",
    "At a high level, LeNet (LeNet-5) consists of two parts:\n",
    "\n",
    "(i) A convolutional encoder consisting of two convolutional layers.\n",
    "\n",
    "(ii) A dense block consisting of three fully-connected layers.\n",
    "\n",
    "Below is the data flow in LeNet, the input is a handwritten digit, the output a probability over 10 possible outcomes:\n",
    "<img src=\"img/lenet.png\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "We took a small liberty with the original LeNet-5 model by removing the Gaussian activation in the final layer.\n",
    "Other than that, our LeNet architechture should match the original LeNet-5 architecture.\n",
    "\n",
    "Below is our LeNet architechture with more structural details. You may check this figure below carefully before you start your LeNet implementation.\n",
    "\n",
    "<img src=\"img/lenet-vert.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "From the structual perspective, note that the height and width of the output of each layer throughout the convolutional block are reduced (compared with the previous layer):\n",
    "\n",
    "The first convolutional layer uses 2 pixels of padding to compensate for the reduction in height and width that would otherwise result from using a $5 \\times 5$ kernel.\n",
    "\n",
    "The second convolutional layer forgoes padding, and thus the height and width are both reduced by 4 pixels. \n",
    "\n",
    "As we go up the stack of layers, the number of channels increases layer-over-layer from 1 in the input to 6 after the first convolutional layer and 16 after the second convolutional layer.\n",
    "\n",
    "Meanwhile, each pooling layer halves the height and width.\n",
    "\n",
    "Finally, each fully-connected layer reduces dimensionality, emitting an output whose dimension matches the total number of classes in our classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build LeNet and train this CNN using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "### We provide you this file so that you can test and debug your task 3 implementations in your .py file by yourself, you don't need to modify any code in this jupyter notebook, but you are welcome to change implementations in this notebook if you want to play with this example model training process.\n",
    "    \n",
    "### You don't need to submit this notebook.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use automatic reloading for your code from task3-template.py\n",
    "# remember to rename task3-template.py before you submit it.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from task3_template import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load your LeNet-5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_lenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 6,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X = torch.randn(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "print(\"The output data shapes of all layers in your lenet are:\")\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape: \\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The output data shapes of all layers in correct lenet are:\")\n",
    "print(get_correct_lenet_shape_str())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the two cells above, all the layer names, and the output data shapes of all the layers should be the same. \n",
    "#### (Please do not modify the layer names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "###  2. Train your LeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "Now let us train and evaluate the LeNet-5 model using your implementation.\n",
    "\n",
    "We recommend using Deep Learning accleration hardware (like CUDA compatible GPU). However, if you are using Mac or other computer without CUDA GPU, it is okay. After all, you only need to run LeNet training for 10 epochs.\n",
    "\n",
    "If you are running without Deep Learning accleration hardware, we recommend that you print out test batch accuracy for every batch in every epoch in the training process, which might make things easier for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 19,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "lr, num_epochs = 1.1, 10\n",
    "# We recommend not to start training before you have got the correct LeNet-5 model\n",
    "train_cnn(net, train_iter, test_iter, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_accuracy = evaluate_accuracy_try_gpu(net, test_iter)\n",
    "print(\"After model training, your LeNet-5 achieves test dataset accuracy:\",test_dataset_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 10 epochs of training with learning rate 0.9~1.1, batch size 256, a test dataset accuracy(on all 10000 test samples) >0.7000 is usually considered correct and will give you a full score for train_cnn function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing task 3!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
