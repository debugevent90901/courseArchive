{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "# ECE449 Machine Learning - Assignment 2\n",
    "\n",
    "## Task 1: Simple and Small DNN from scratch\n",
    "\n",
    "\n",
    "[//]: # \"Notebook Created by Jinghua Wang (jinghuawang@intl.zju.edu.cn), last modified on 2020-11-03\"\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple Deep Neural Network(DNN, and more specifically, a multi-layer perceptron, MLP) model with input dimension = 3 and output dimension = 1, this DNN has 1 hidden layer, and the activation function is ReLu:\n",
    "<img src=\"img/simple_DNN.png\" width=\"500\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's ignore bias in this DNN.\n",
    "\n",
    "We have: \n",
    "\n",
    "$O = k_1 \\times a_1 + k_2 \\times a_2$\n",
    "\n",
    "$a_1 = ReLu(h_1)$,\n",
    "$a_2 = ReLu(h_2)$ \n",
    "\n",
    "$h_1 = x_1 \\times w_{11}+x_2 \\times w_{12}+x_3 \\times w_{13}$ \n",
    "\n",
    "$h_2 = x_2 \\times w_{21}+x_2 \\times w_{22}+x_3 \\times w_{23}$ \n",
    "\n",
    "In vector-matrix form, we have:\n",
    "\n",
    "$X = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$,\n",
    "$H = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}$,\n",
    "$A = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix}$,\n",
    "$W = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{bmatrix}$,\n",
    "$K = \\begin{bmatrix} k_1 & k_2 \\end{bmatrix}$ \n",
    "\n",
    "$H = W \\times X$,\n",
    "$A = ReLu(H)$,\n",
    "$O = K \\times A$\n",
    "\n",
    "$O = K \\times ReLu(W \\times X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the label true value is $Y$ given input $\\vec{X} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$, and we use squared loss function $L = \\frac{1}{2}(O - y)^2$.\n",
    "\n",
    "$W$ and $K$ contain all the parameters (8 numbers in total) we need to update in the training process.\n",
    "\n",
    "Before training starts, we assign some random initial values to our parameters $W$ and $K$.\n",
    "\n",
    "During training, we obtain $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial K}$, then we use $W_i = W_{i-1} - \\frac{\\partial L}{\\partial W} \\times lr$ and $K_i = K_{i-1} - \\frac{\\partial L}{\\partial K} \\times lr$ to update $W$ and $K$ in order to make loss L smaller and smaller.\n",
    "\n",
    "This is the basic idea of gradient decent, which is a popular set of non-convex optimization methods used in machine learning especially in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the expressions for the gradients of our 6 parameters in $W$ and 2 parameters in $K$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function is: $L = \\frac{1}{2}(O-y)^2 = \\frac{1}{2}(K \\times A - y)^2 = \\frac{1}{2}((k_1 \\times a_1 + k_2 \\times a_2) - y)^2$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial K} = \n",
    "\\begin{bmatrix} \\frac{\\partial L}{\\partial k_1} \\\\ \\frac{\\partial L}{\\partial k_2} \\end{bmatrix} = \n",
    "\\begin{bmatrix} \\frac{\\partial L}{\\partial O} \\frac{\\partial O}{\\partial k_1} \\\\ \\frac{\\partial L}{\\partial O} \\frac{\\partial O}{\\partial k_2} \\end{bmatrix} = \n",
    "\\begin{bmatrix} (O-y) a_1 \\\\ (O-y) a_2 \\end{bmatrix} = \n",
    "\\begin{bmatrix}a_1 \\\\ a_2\\end{bmatrix}(O-y) = A(O-y)$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W}$ = \n",
    "$\\begin{bmatrix} \n",
    "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial w_{13}} \\\\ \n",
    "\\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{22}} & \\frac{\\partial L}{\\partial w_{23}} \n",
    "\\end{bmatrix}$ = \n",
    "$\\begin{bmatrix} \n",
    "  \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial a_1}\\frac{\\partial a_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial w_{11}} \n",
    "& \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial a_1}\\frac{\\partial a_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial w_{12}} \n",
    "& \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial a_1}\\frac{\\partial a_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial w_{13}} \\\\ \n",
    "  \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial a_2}\\frac{\\partial a_2}{\\partial h_2}\\frac{\\partial h_2}{\\partial w_{21}} \n",
    "& \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial a_2}\\frac{\\partial a_2}{\\partial h_2}\\frac{\\partial h_2}{\\partial w_{22}} \n",
    "& \\frac{\\partial L}{\\partial O}\\frac{\\partial O}{\\partial a_2}\\frac{\\partial a_2}{\\partial h_2}\\frac{\\partial h_2}{\\partial w_{23}} \n",
    "\\end{bmatrix}$ =\n",
    "$\\begin{bmatrix} \n",
    "  (O-y)k_1\\frac{\\partial ReLu(h_1)}{\\partial h_1}x_1 \n",
    "& (O-y)k_1\\frac{\\partial ReLu(h_1)}{\\partial h_1}x_2\n",
    "& (O-y)k_1\\frac{\\partial ReLu(h_1)}{\\partial h_1}x_3 \\\\ \n",
    "  (O-y)k_2\\frac{\\partial ReLu(h_2)}{\\partial h_2}x_1\n",
    "& (O-y)k_2\\frac{\\partial ReLu(h_2)}{\\partial h_2}x_2\n",
    "& (O-y)k_2\\frac{\\partial ReLu(h_2)}{\\partial h_2}x_3\n",
    "\\end{bmatrix}$ = \n",
    "$(O-y)\\begin{bmatrix} \n",
    "k_1\\frac{\\partial ReLu(h_1)}{\\partial h_1} \\\\ \n",
    "k_2\\frac{\\partial ReLu(h_2)}{\\partial h_2}\n",
    "\\end{bmatrix} \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix}$ =\n",
    "$\\begin{bmatrix}\n",
    "k_1 & 0 \\\\\n",
    "0 & k_2 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial ReLu(h_1)}{\\partial h_1} \\\\ \n",
    "\\frac{\\partial ReLu(h_2)}{\\partial h_2}\n",
    "\\end{bmatrix} \n",
    "(O-y)\n",
    "\\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use numpy to build the simple DNN we have seen above, and we use gradient decent method to train this DNN to fit a randomly generated dataset for a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "### We provide you this file so that you can test and debug your task 1 implementations in your .py file by yourself, you don't need to modify any code in this jupyter notebook, but you are welcome to change implementations in this notebook if you want to play with this example model training process.\n",
    "    \n",
    "### You don't need to submit this notebook.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use automatic reloading for your code from task1-template.py\n",
    "# remember to rename task1-template.py before you submit it.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from task1_template import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Random Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random dataset and visualize it.\n",
    "num_samples = 400\n",
    "dataset = generate_random_dataset(num_samples)\n",
    "visualize_dataset(dataset, window_title=\"Random Generated Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset and visualize the normalized dataset.\n",
    "dataset = normalize_dataset(dataset)\n",
    "visualize_dataset(dataset, window_title=\"Normalized Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Your Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Test Your ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Your relu figure should be the same as the correct figure shown in the right.\")\n",
    "# test your relu implementation\n",
    "relu_test_x = np.array([-1,-0.5,0,0.5,1])\n",
    "relu_test_y = np.array([0,0,0,0.5,1])\n",
    "fig, ax = plt.subplots(1,2, figsize=(9,4))\n",
    "ax[0].set_title(\"Your ReLu\")\n",
    "ax[0].plot(relu_test_x, relu(relu_test_x))\n",
    "ax[0].set_ylim([-0.75,1.25])\n",
    "ax[1].set_title(\"Correct ReLu\")\n",
    "ax[1].plot(relu_test_x, relu_test_y)\n",
    "ax[1].set_ylim([-0.75,1.25])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test Your $\\frac{\\partial ReLu(x)}{\\partial x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Your prelu figure should be the same as the correct figure shown in the right.\")\n",
    "# test your prelu implementation\n",
    "prelu_test_x = np.array([-1,-0.5,0,0.0000000000001,0.5,1])\n",
    "prelu_test_y = np.array([0,0,0,1,1,1])\n",
    "fig, ax = plt.subplots(1,2, figsize=(9,4))\n",
    "ax[0].set_title(\"Your prelu\")\n",
    "ax[0].plot(prelu_test_x, prelu(prelu_test_x))\n",
    "ax[0].set_ylim([-0.5,1.5])\n",
    "ax[1].set_title(\"Correct prelu\")\n",
    "ax[1].plot(prelu_test_x, prelu_test_y)\n",
    "ax[1].set_ylim([-0.5,1.5])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Test Your $\\frac{\\partial Loss}{\\partial W}$ and $\\frac{\\partial Loss}{\\partial K}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pLpW_func(W):\n",
    "    _,_, O = DNNforward(dataset[:,:3], W, K_test)\n",
    "    return loss(dataset[:,3], O)\n",
    "\n",
    "def pLpK_func(K):\n",
    "    _,_, O = DNNforward(dataset[:,:3], W_test, K)\n",
    "    return loss(dataset[:,3], O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_test = np.random.uniform(low=-0.2, high=0.2, size=(2,3))\n",
    "K_test = np.random.uniform(low=-0.2, high=0.2, size=(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pLpW_approx = computeNumericalGradient(pLpW_func, W_test)\n",
    "pLpK_approx = computeNumericalGradient(pLpK_func, K_test)\n",
    "\n",
    "H_test,A_test,O_test = DNNforward(dataset[:,:3], W_test, K_test)\n",
    "pLpW_yours, pLpK_yours = DNNbackward(dataset[:,:3], dataset[:,3], W_test, K_test, H_test, A_test, O_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The approximated gradient pLpW is:\\n\", pLpW_approx, '\\n')\n",
    "print(\"The gradient pLpW calculated by your implementation is:\\n\", pLpW_yours,'\\n')\n",
    "print(\"The terms in the two matrices should be almost the same and non-zero (small differences in the last 1-2 digits are accepted).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The approximated gradient pLpK is:\\n\", pLpK_approx, '\\n')\n",
    "print(\"The gradient pLpK calculated by your implementation is:\\n\", pLpK_yours,'\\n')\n",
    "print(\"The terms in the two vectors should be almost the same and non-zero (small differences in the last 1-2 digits are accepted).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on getting all these structural functions correct! Next let's train your DNN model and see if your train function works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the initial values of W and K by uniform distribution.\n",
    "rand_weight_low, rand_weight_high = -0.2, 0.2 \n",
    "\n",
    "np.random.seed(int(time.time()))\n",
    "W = np.random.uniform(low=rand_weight_low, high=rand_weight_high, size=(2,3))\n",
    "K = np.random.uniform(low=rand_weight_low, high=rand_weight_high, size=(2))\n",
    "print(\"The initial W is:\\n\")\n",
    "print(W)\n",
    "print(\"\\nThe initial K is:\\n\")\n",
    "print(K)\n",
    "\n",
    "print(\"Before training, the train accuracy is:\", trainAcc(dataset[:,:3], dataset[:,3], W, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_list, K_list = train(dataset[:,:3], dataset[:,3], W, K, lr=0.005, num_epochs=50, batch_size=20, print_loss=False)\n",
    "W_trained, K_trained = W_list[-1], K_list[-1]\n",
    "print(\"The last trained W is:\\n\")\n",
    "print(W_trained)\n",
    "print(\"The last trained K is:\\n\")\n",
    "print(K_trained)\n",
    "print(\"After training, the train accuracy is:\", trainAcc(dataset[:,:3], dataset[:,3], W_trained, K_trained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After training, your DNN train accuracy should be almost 100% after training using our default settings. If your train accuracy is below 95% we suggest you check your implementations again, particularly your implementations in the train function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing task 1! Below is an optional task for visualizing the model inference boundary of your DNN during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this code block requires a correct DNNforward implementation in task 1 to work correctly\n",
    "total_epochs = len(W_list)\n",
    "interact(plotDNNInferenceEpoch, \n",
    "         epoch_idx = widgets.IntSlider(total_epochs-1, 0, total_epochs-1, 1),\n",
    "         dataset = fixed(dataset),\n",
    "         W_list = fixed(W_list),\n",
    "         K_list = fixed(K_list),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
